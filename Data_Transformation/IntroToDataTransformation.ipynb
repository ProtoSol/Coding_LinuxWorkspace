{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization, or mean removal and variance scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the [SciKit - Learn Guide](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization is essential for many machine learning models in scikit-learn, as models can perform poorly if features do not resemble a standard normal distribution (zero mean, unit variance). Typically, this involves centering the data by removing the mean and scaling it by the standard deviation of each feature. If a feature has a much larger variance than others, it can dominate the learning process, hindering the model's ability to learn effectively from other features. The StandardScaler [-3 to +3] utility class in the preprocessing module helps achieve this standardization. The other method being Scaling to a known range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Examples of Scaling to a known range are as follows :-\n",
    "1. Min Max Scaler (0 - 1)\n",
    "2. Max Absolute Scaler (-1 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "`StandardScaler` is a preprocessing tool in scikit-learn that standardizes features by removing the mean and scaling them to unit variance. This process transforms the data into a standard normal distribution, which is crucial for many machine learning algorithms to perform optimally.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "The standard score of a sample $x$ is calculated using the formula:\n",
    "\n",
    "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
    "\n",
    "where:\n",
    "- $\\mu$ is the mean of the training samples.\n",
    "- $\\sigma$ is the standard deviation of the training samples.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Centering and Scaling**: Centers the data by subtracting the mean and scales it by the standard deviation, ensuring that each feature contributes equally to the model.\n",
    "- **Independence**: The centering and scaling are performed independently for each feature, which is essential in high-dimensional datasets.\n",
    "- **Handling Sparse Data**: Can be applied to sparse matrices by setting `with_mean=False` to maintain the sparsity of the dataset.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `copy`: If `True`, it creates a copy of the data. If `False`, it tries to perform the operation in-place.\n",
    "- `with_mean`: If `True`, it centers the data before scaling. This option is not available for sparse matrices.\n",
    "- `with_std`: If `True`, it scales the data to unit variance.\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "Hereâ€™s a simple example of how to use `StandardScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1.]\n",
      " [-1. -1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantile transformation is a method used in data preprocessing to transform the features to follow a uniform or normal distribution. This non-linear transformation technique is particularly useful when dealing with data that has outliers or when the distribution of the data is not Gaussian.\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. **Purpose**: \n",
    "   - Reduces the impact of outliers\n",
    "   - Makes the data distribution more Gaussian-like or uniform\n",
    "   - Can improve the performance of many machine learning algorithms\n",
    "\n",
    "2. **Process**:\n",
    "   - Ranks the data\n",
    "   - Transforms the features to follow a specified distribution (usually uniform or normal)\n",
    "\n",
    "3. **Types**:\n",
    "   - Uniform quantile transformation\n",
    "   - Gaussian quantile transformation\n",
    "\n",
    "4. **Mathematical Representation**:\n",
    "   For a feature $X$ with cumulative distribution function $F_X$, the quantile transformation $Q$ is:\n",
    "   \n",
    "   $Q(X) = F^{-1}(F_X(X))$\n",
    "\n",
    "   where $F^{-1}$ is the inverse cumulative distribution function of the desired output distribution.\n",
    "\n",
    "5. **Advantages**:\n",
    "   - Robust to outliers\n",
    "   - Preserves order and relative distances between data points\n",
    "   - Can handle non-linear relationships in the data\n",
    "\n",
    "6. **Considerations**:\n",
    "   - May change the interpretability of the features\n",
    "   - Can be computationally expensive for large datasets\n",
    "   - May not be suitable if the original scale of the data is important\n",
    "\n",
    "7. **Implementation in Python**:\\\n",
    "   Scikit-learn provides `QuantileTransformer` for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import QuantileTransformer   \n",
    "# qt = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "# X_transformed = qt.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantile transformation can be a powerful tool in your data preprocessing toolkit, especially when dealing with skewed or non-Gaussian data distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping to Gausissian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping data to a Gaussian (or normal) distribution is a common preprocessing technique in machine learning and statistics. This process, also known as Gaussian transformation or normalization, can be beneficial for many algorithms that assume normally distributed input features.\n",
    "\n",
    "### Why Map to Gaussian?\n",
    "\n",
    "- Many statistical methods and machine learning algorithms assume Gaussian distributed input data.\n",
    "- It provides a way to standardize features to a common scale.\n",
    "- Can help in reducing the impact of outliers.\n",
    "- Often leads to better performance in various machine learning models.\n",
    "\n",
    "### Methods for Gaussian Mapping\n",
    "\n",
    "#### 1. Box-Cox Transformation\n",
    "\n",
    "The Box-Cox transformation is defined as:\n",
    "\n",
    "$$\n",
    "y(\\lambda) = \n",
    "\\begin{cases} \n",
    "\\frac{x^\\lambda - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0 \\\\\n",
    "\\ln(x), & \\text{if } \\lambda = 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where \\( x \\) is the original data and \\( \\lambda \\) is the transformation parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import boxcox\n",
    "# transformed_data, lambda_param = boxcox(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Yeo-Johnson Transformation\n",
    "\n",
    "The Yeo-Johnson transformation is similar to Box-Cox but can handle negative values. It is defined as follows:\n",
    "\n",
    "$$\n",
    "y(\\lambda) =\n",
    "\\begin{cases}\n",
    "\\frac{(x+1)^\\lambda - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0, \\, x \\geq 0 \\\\\n",
    "\\ln(x+1), & \\text{if } \\lambda = 0, \\, x \\geq 0 \\\\\n",
    "-\\frac{(-x+1)^{2-\\lambda} - 1}{2-\\lambda}, & \\text{if } \\lambda \\neq 2, \\, x < 0 \\\\\n",
    "-\\ln(-x+1), & \\text{if } \\lambda = 2, \\, x < 0 \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PowerTransformer\n",
    "# pt = PowerTransformer(method='yeo-johnson')\n",
    "# transformed_data = pt.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`What is Normalization?`**\n",
    "\n",
    "Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.\n",
    "The goal of normalization is to ensure that all samples are on the same scale, which can improve the\n",
    "performance of many machine learning algorithms. For example, the Euclidean distance between two points is sensitive to\n",
    "the scale of the data. If the data is not normalized, the distance between two points can be\n",
    "dominated by the scale of the data rather than the actual difference between the points.\n",
    "\n",
    "**`How to Normalize Data?`**\n",
    "\n",
    "There are several ways to normalize data, including:\n",
    "1.  **Standardization**: This involves subtracting the mean and dividing by the standard deviation for\n",
    "each feature. This is also known as Z-scoring.\n",
    "2.  **L1 Normalization**: This involves dividing each feature by the sum of its absolute\n",
    "values. This is also known as L1 normalization or sum-normalization.\n",
    "3.  **L2 Normalization**: This involves dividing each feature by its Euclidean norm (\n",
    "4.  **Min-Max Scaling**: This involves scaling each feature to a common range, usually\n",
    "between 0 and 1.\n",
    "\n",
    "**`Why is Normalization Important?`**\n",
    "\n",
    "**Normalization is important because it can improve the performance of many machine learning algorithms. Here are some reasons\n",
    "why normalization is important:**\n",
    "1.  **Improved Performance**: Normalization can improve the performance of many machine learning algorithms, such\n",
    "as k-means clustering, k-nearest neighbors, and support vector machines.\n",
    "2.  **Reduced Overfitting**: Normalization can reduce overfitting by preventing features\n",
    "with large ranges from dominating the model.\n",
    "3.  **Improved Interpretability**: Normalization can improve the interpretability of the results by ensuring\n",
    "that all features are on the same scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeseries_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
